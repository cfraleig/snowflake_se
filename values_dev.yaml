# Default values for Airflow.
fullnameOverride: ""
nameOverride: ""
airflowVersion: "2.7.3"

# Images
images:
  airflow:
    repository: 731793770384.dkr.ecr.us-east-1.amazonaws.com/airflow
    tag: active_dev
    pullPolicy: Always
  pgbouncer:
    repository: apache/airflow
    tag: airflow-pgbouncer-2023.02.24-1.16.1
    pullPolicy: IfNotPresent
  gitSync:
    repository: registry.k8s.io/git-sync/git-sync
    tag: v4.1.0
    pullPolicy: IfNotPresent

# Airflow executor
executor: "KubernetesExecutor"

# set extra envars
extraEnv: |
  - name: POSTGRES_PASSWORD
    valueFrom:
      secretKeyRef:
        name: airflow-secrets  # Name of the Kubernetes Secret
        key: postgres_password   # Key within the Kubernetes Secret
  - name: POSTGRES_USER
    valueFrom:
      secretKeyRef:
        name: airflow-secrets  # Name of the Kubernetes Secret
        key: postgres_username   # Key within the Kubernetes Secret
  - name: GITHUB_OAUTH_APP_SECRET
    valueFrom:
      secretKeyRef:
        name: airflow-secrets  # Name of the Kubernetes Secret
        key: GITHUB_OAUTH_APP_SECRET   # Key within the Kubernetes Secret
  - name: GITHUB_OAUTH_APP_ID
    valueFrom:
      secretKeyRef:
        name: airflow-secrets  # Name of the Kubernetes Secret
        key: GITHUB_OAUTH_APP_ID   # Key within the Kubernetes Secret
  - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
    valueFrom:
      secretKeyRef:
        name: airflow-secrets  # Name of the Kubernetes Secret
        key: sql_alchemy_conn   # Key within the Kubernetes Secret
  - name: OKTA_CLIENT_ID
    valueFrom:
      secretKeyRef:
        name: airflow-secrets  # Name of the Kubernetes Secret
        key: okta_client_id   # Key within the Kubernetes Secret
  - name: OKTA_CLIENT_SECRET
    valueFrom:
      secretKeyRef:
        name: airflow-secrets  # Name of the Kubernetes Secret
        key: okta_client_secret   # Key within the Kubernetes Secret

# built in envars
enableBuiltInSecretEnvVars:
  AIRFLOW__CORE__FERNET_KEY: true
  # For Airflow <2.3, backward compatibility; moved to [database] in 2.3
  AIRFLOW__WEBSERVER__SECRET_KEY: true
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: false
  AIRFLOW_CONN_AIRFLOW_DB: false

# set static secret name for fernet and webserver secret keys
fernetKeySecretName: airflow-secrets
webserverSecretKeySecretName: airflow-secrets

#----------------------------------------------------
# Git sync
#----------------------------------------------------
dags:
  persistence:
    enabled: false
  gitSync:
    enabled: true
    repo: ssh://git@github.com/puradev/airflow.git
    branch: main
    rev: HEAD
    depth: 1
    maxFailures: 0
    subPath: "/dags"
    sshKeySecret: airflow-git-ssh-secret
    #the ssh key secret needs to be configured using kubectl to load the ssh key from file else its encoding gets screwy: https://blog.devgenius.io/setting-up-apache-airflow-on-kubernetes-with-gitsync-beaac2e397f3

    # This is mandatory for gitSync feature
    # Checkout the docs for creating knownHosts key for Github https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#knownhosts
    knownHosts: |
      github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl
      github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=
      github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=

    # Adjust the resources according to your workloads
    resources:
      limits:
        cpu: "1"
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 128Mi

#----------------------------------------------------
# Airflow webserver settings
#----------------------------------------------------

# TODO: Add webserverConfig with Github Auth
webserver:
  # Number of webservers
  replicas: 1
  serviceAccount:
    create: false
  resources:
    limits:
      cpu: 2
      memory: 2Gi
    requests:
      cpu: 200m
      memory: 2Gi
  allowPodLogReading: true
  livenessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 60
    failureThreshold: 20
    periodSeconds: 5

  readinessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 60
    failureThreshold: 20
    periodSeconds: 5

  #Configuring Ingress for Airflow WebUi hence the service type is changed to NodePort
  service:
    type: NodePort
    ports:
    - name: airflow-ui
      port: "{{ .Values.ports.airflowUI }}"

  # This string (can be templated) will be mounted into the Airflow Webserver as a custom
  # webserver_config.py. You can bake a webserver_config.py in to your image instead.
  webserverConfig: |
    from flask_appbuilder.security.manager import AUTH_OAUTH
    import os
    AUTH_TYPE = AUTH_OAUTH
    AUTH_ROLES_SYNC_AT_LOGIN = False  # Checks roles on every login
    AUTH_USER_REGISTRATION = True  # allow users who are not already in the FAB DB to register
    AUTH_USER_REGISTRATION_ROLE = "Admin"

    # Define your OAuth configs

    pura_url = 'https://pura.okta.com/'
    OAUTH_PROVIDERS = [{
        "name": "okta",
        "icon": "fa-circle-o",
        "token_key": "access_token",
        "remote_app": {
            "client_id": os.getenv('OKTA_CLIENT_ID'),
            "client_secret": os.getenv('OKTA_CLIENT_SECRET'),
            "api_base_url": f"{pura_url}oauth2/v1/",
            "client_kwargs": {"scope": "openid profile email groups"},
            "access_token_url": f"{pura_url}oauth2/v1/token",
            "authorize_url": f"{pura_url}oauth2/v1/authorize",
            "server_metadata_url": f"{pura_url}.well-known/openid-configuration",
            }
    }]

#----------------------------------------------------
# Ingress configuration with AWS LB Controller
# Checkout this doc for more annotations https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/
#----------------------------------------------------
ingress:
  web:
    enabled: true
    annotations:
      alb.ingress.kubernetes.io/group.name: dataengineering
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/scheme: internet-facing
      # The following two lines alone allow for HTTP -> HTTPS redirection
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
      alb.ingress.kubernetes.io/ssl-redirect: '443'
      alb.ingress.kubernetes.io/healthcheck-path: '/health'
      # Note: certificate is currently manually added
      # alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:731793770384:certificate/ac525d9d-5729-42c9-a4e1-7dd3acbd227e
      alb.ingress.kubernetes.io/ip-address-type: 'dualstack'
      external-dns.alpha.kubernetes.io/hostname: airflow.dev.trypura.io

    path: '/'
    # The pathType for the above path (used only with Kubernetes v1.19 and above)
    pathType: "Prefix"
    # The hostnames or hosts configuration for the web Ingress
    hosts:
    - name: "airflow.dev.trypura.io"
      tls:
        # Enable TLS termination for the web Ingress
        enabled: true
        # the name of a pre-created Secret containing a TLS private key and certificate
        secretName: ""
    ingressClassName: alb

  #----------------------------------------------------
  # Database settings
  #----------------------------------------------------
  # database connection is managed by the envar AIRFLOW__DATABASE__SQL_ALCHEMY_CONN


#----------------------------------------------------
# StatsD settings
#----------------------------------------------------
statsd:
  enabled: false
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 1
      memory: 512Mi

#----------------------------------------------------
# PgBouncer settings
#----------------------------------------------------
pgbouncer:
  enabled: false
  auth_type: scram-sha-256

#----------------------------------------------------
# Disable local postgresql for external RDS implementation
#----------------------------------------------------
postgresql:
  enabled: false

#----------------------------------------------------
# Config
#----------------------------------------------------
config:
  core:
    dags_folder: '{{ include "airflow_dags" . }}'
    load_examples: 'False'
    executor: '{{ .Values.executor }}'
    colored_console_log: 'True'
    remote_logging: 'True'
    max_active_runs_per_dag: 1
    max_active_tasks_per_dag: 25
    parallelism: 100
  # Logging configured to S3 bucket. You can replace the bucket name with your own
  # TODO: replace this with existing logging info
  logging:
    remote_logging: 'True'
    logging_level: 'INFO'
    colored_console_log: 'True'
    remote_base_log_folder: "s3://pura-data-platform-airflow/airflow-logs"
    # aws_s3_conn is the name of the connection that needs to be created using Airflow admin UI once the deployment is complete
    # Steps can be seen in the docs link here -> https://github.com/apache/airflow/issues/25322
    remote_log_conn_id: 'aws_s3_conn'
    delete_worker_pods: 'True'
    encrypt_s3_logs: 'True'
  metrics:
    statsd_on: 'True'
    statsd_port: 8125
    statsd_prefix: airflow
    statsd_host: datadog.default
    statsd_datadog_enabled: 'True'
    statsd_datadog_metrics_tags: 'True'
  webserver:
    base_url: 'https://airflow.dev.trypura.io'
    enable_proxy_fix: 'True'
    rbac: 'True'
  scheduler:
    standalone_dag_processor: '{{ ternary "True" "False" .Values.dagProcessor.enabled }}'
    run_duration: 41460
    catch_up_by_default: 'False'
  kubernetes_executor:
    namespace: '{{ .Release.Namespace }}'
    multi_namespace_mode: 'False'
    delete_worker_pods: 'True'

#----------------------------------------------------
# Airflow Worker Config
#----------------------------------------------------
workers:
  serviceAccount:
    create: false
  persistence:
    enabled: false
  resources:
    limits:
      cpu: 2
      memory: 2Gi
    requests:
      cpu: 200m
      memory: 256Mi
#----------------------------------------------------
# Airflow scheduler settings
#----------------------------------------------------
scheduler:
  replicas: 1
  serviceAccount:
    create: false
  resources:
    limits:
      cpu: 2
      memory: 2Gi
    requests:
      cpu: 200m
      memory: 512Mi

#-----------------------------------------------------
# Triggerer
#-----------------------------------------------------

triggerer:
  enabled: true
  persistence:
    # Enable persistent volumes
    enabled: false
  logGroomerSidecar:
    # Whether to deploy the Airflow triggerer log groomer sidecar.
    enabled: false
